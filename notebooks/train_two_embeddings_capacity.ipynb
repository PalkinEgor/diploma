{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport json\nimport torch.nn.functional as F\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATASET_NAME = 'databricks/databricks-dolly-15k'\nMODEL_NAME = 'Qwen/Qwen2.5-0.5B'\nRANDOM_STATE = 42\nMAXITER = 500\nMIN_WORDS = 10\nMAX_WORDS = 100\nSAMPLE_SIZE = 50\nBATCH_SIZE = 8\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nHYPERPARAMS = {\n        'lr': 0.01,\n        'weight_decay': 0.01,\n        'betas': (0.9, 0.9)\n    }\ntorch.manual_seed(RANDOM_STATE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, texts):\n        self.texts = texts\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        return self.texts[idx], idx\n\ndef evaluate_plain_model(model, input_ids):\n    model.eval()\n    with torch.no_grad():\n        logits = model(input_ids).logits\n        vocab_size = logits.shape[-1]\n        \n        logits_for_loss = logits[:, :-1, :].reshape((-1, vocab_size))\n        targets_for_loss = input_ids[:, 1:].reshape(-1)\n\n        loss = F.cross_entropy(logits_for_loss, targets_for_loss).item()\n        pred = logits[:, :-1, :].argmax(dim=-1).reshape(-1)\n\n        accuracy, seq_accuracy, correct_prefix_length = calculate_metrics(targets_for_loss, pred)\n        return loss, accuracy, seq_accuracy, correct_prefix_length\n\ndef generate_input_one(vectors, text_length):\n    return torch.cat([vectors[:1, None, :], vectors[1:2, None, :].expand(-1, text_length - 1, -1)], dim=1)\n\ndef generate_input(batch_vectors, lengths):\n    embeds = []\n    for vectors, length in zip(batch_vectors, lengths):\n        embeds.append(generate_input_one(vectors, length).squeeze(0))\n    return pad_sequence(embeds, batch_first=True, padding_value=0.0)   \n\ndef calculate_metrics(target, pred):\n    accuracy = (pred == target).float().mean().item()\n    first_wrong = torch.nonzero((pred != target).float())\n    if first_wrong.shape[0] == 0:\n        seq_accuracy = 1.0\n        correct_prefix_length = len(pred)\n    else:\n        seq_accuracy = first_wrong[0].item() / len(pred)\n        correct_prefix_length = first_wrong[0].item()\n    return accuracy, seq_accuracy, correct_prefix_length\n\ndef collate_fn(batch, tokenizer, device):\n    texts = [item[0] for item in batch]\n    indices = [item[1] for item in batch]\n    input_ids = [tokenizer.encode(text, return_tensors='pt').reshape(-1) for text in texts]\n    lengths = [text.shape[0] for text in input_ids]\n    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n    attention_mask = (input_ids != tokenizer.pad_token_id).int()\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'lengths': lengths,\n        'indices': indices\n    }\n\ndef safe_sample(group, n, seed):\n    if len(group) < n:\n        return group\n    return group.sample(n, random_state=seed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = load_dataset(DATASET_NAME)\ndf = dataset['train'].to_pandas()\ndf = df[df['response'].apply(lambda x: len(x.split(' ')) > MIN_WORDS)]\ndf = df.groupby(by='category', group_keys=False).apply(lambda x: safe_sample(x, SAMPLE_SIZE, RANDOM_STATE)).reset_index(drop=True)\ntexts = list(df['response'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt  \ntext_len = [len(text.split(' ')) for text in texts]\nplt.hist(text_len, bins=25)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"texts = [' '.join(text.split(' ')[:MAX_WORDS]) if len(text.split(' ')) > MAX_WORDS else text for text in texts]\ntext_len = [len(text.split(' ')) for text in texts]\nplt.hist(text_len, bins=25)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntext_dataset = TextDataset(texts)\ntext_dataloader = DataLoader(text_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda x: collate_fn(x, tokenizer, DEVICE))\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result = []\n\nfor batch in tqdm(text_dataloader, desc='Processing dataset'):\n    tokenized_text = batch['input_ids']\n    lengths = batch['lengths']\n    attention_mask = batch['attention_mask']\n    indices = batch['indices']\n    labels = tokenized_text.clone()\n        \n    vectors = torch.nn.Parameter(torch.randn(BATCH_SIZE, 2, model.config.hidden_size, device=DEVICE))\n    optimizer = torch.optim.AdamW([vectors], lr=HYPERPARAMS['lr'], betas=HYPERPARAMS['betas'], weight_decay=HYPERPARAMS['weight_decay'])\n\n    # vanilla_loss, vanilla_accuracy, vanilla_seq_accuracy, vanilla_correct_prefix_length = evaluate_plain_model(model, tokenized_text)\n    max_accuracy = torch.zeros(BATCH_SIZE, device=DEVICE)\n    max_seq_accuracy = torch.zeros(BATCH_SIZE, device=DEVICE)\n    best_vectors = [None] * BATCH_SIZE\n    best_metrics = [(0.0, 0.0, 0)] * BATCH_SIZE\n        \n    for _ in range(MAXITER):\n        optimizer.zero_grad()\n            \n        current_input = generate_input(vectors, lengths)\n        logits = model(inputs_embeds=current_input, attention_mask=attention_mask).logits\n        logits = logits.reshape(-1, logits.shape[-1])\n        loss = torch.nn.functional.cross_entropy(logits, labels.reshape(-1), ignore_index=tokenizer.pad_token_id)\n\n        pred = logits.argmax(dim=-1).view(tokenized_text.shape)\n        for i in range(BATCH_SIZE):\n            current_pred = pred[i, :lengths[i]]\n            current_labels = labels[i, :lengths[i]]\n            accuracy, seq_accuracy, correct_prefix_length = calculate_metrics(current_labels, current_pred)\n            if (accuracy > max_accuracy[i]) or (accuracy == max_accuracy[i] and seq_accuracy > max_seq_accuracy[i]):\n                max_accuracy[i] = accuracy\n                max_seq_accuracy[i] = seq_accuracy\n                best_metrics[i] = (accuracy, seq_accuracy, correct_prefix_length)\n                best_vectors[i] = vectors[i].detach().clone()\n            \n        loss.backward()\n        optimizer.step()\n        \n    for i in range(BATCH_SIZE):\n        result.append({\n            'instruction': df.iloc[indices[i]]['instruction'],\n            'context': df.iloc[indices[i]]['context'],\n            'category': df.iloc[indices[i]]['category'],\n            'text': df.iloc[indices[i]]['response'],\n            'accuracy': best_metrics[i][0],\n            'seq_accuracy': best_metrics[i][1],\n            'correct_prefix_len': best_metrics[i][2],\n            'best_vectors': best_vectors[i].cpu().numpy().tolist()\n    })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open('/kaggle/working/training_results.json', 'w', encoding='utf-8') as f:\n    json.dump(result, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}