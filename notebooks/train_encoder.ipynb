{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13446537,"sourceType":"datasetVersion","datasetId":8535231}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport torch\nimport pandas as pd\nfrom transformers import BertModel, BertTokenizer\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_NAME = 'bert-base-uncased'\nBATCH_SIZE = 8\nNUM_EPOCHS = 50\nOUTPUT_DIM = 896\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nHYPERPARAMS = {\n        'lr': 0.01,\n        'weight_decay': 0.01,\n        'betas': (0.9, 0.9)\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open('/kaggle/input/diploma-two-vectors/training_results.json', 'r') as f:\n    data = pd.DataFrame(json.load(f))\ndata.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Model(torch.nn.Module):\n    def __init__(self, model_name, output_dim, freeze_bert=True):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        self.regressor = torch.nn.Linear(self.bert.config.hidden_size, output_dim * 2)\n        if freeze_bert:\n            for name, param in self.bert.named_parameters():\n                if not (('layer.9' in name) or ('layer.10' in name) or ('layer.11' in name)):\n                    param.requires_grad = False\n\n    def forward(self, input_ids, attention_mask=None):\n        out = self.bert(input_ids, attention_mask)\n        cls = out.last_hidden_state[:, 0, :]\n        vecs = self.regressor(cls)\n        e, m = torch.split(vecs, vecs.size(1) // 2, dim=1)\n        return e, m        \n\nclass TextDataset(Dataset):\n    def __init__(self, texts, vectors):\n        self.texts = texts\n        self.vectors = vectors\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        return self.texts[idx], self.vectors[idx]\n\ndef collate_fn(batch, tokenizer, device):\n    texts = [item[0] for item in batch]\n    vectors = [torch.tensor(item[1]) for item in batch]\n    input_ids = [tokenizer(text, add_special_tokens=True, return_tensors='pt')['input_ids'].reshape(-1) for text in texts]\n    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n    attention_mask = (input_ids != tokenizer.pad_token_id).long().to(device)\n    vectors = torch.stack(vectors).to(device)\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'vectors': vectors\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Model(MODEL_NAME, OUTPUT_DIM, freeze_bert=False).to(DEVICE)\ntokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\ntext_dataset = TextDataset(list(data['instruction']), list(data['best_vectors']))\ntext_dataloader = DataLoader(text_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda x: collate_fn(x, tokenizer, DEVICE))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=HYPERPARAMS['lr'], betas=HYPERPARAMS['betas'], weight_decay=HYPERPARAMS['weight_decay'])\nloss_fn = torch.nn.MSELoss()\nfor i in range(NUM_EPOCHS):\n    total_loss = 0.0    \n    for batch in text_dataloader:\n        optimizer.zero_grad()\n        \n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        e_target = batch['vectors'][:, 0, :]\n        m_target = batch['vectors'][:, 1, :]\n        \n        e_pred, m_pred = model(input_ids, attention_mask)\n        loss = loss_fn(e_pred, e_target) + loss_fn(m_pred, m_target)\n        total_loss += loss.item() * BATCH_SIZE\n    \n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch: {i + 1}; Loss: {total_loss / 400}')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}