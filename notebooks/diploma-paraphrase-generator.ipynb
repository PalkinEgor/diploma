{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install augmentex","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nfrom augmentex import CharAug\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom tqdm import tqdm\nimport random\nimport json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATASET_NAME = 'databricks/databricks-dolly-15k'\nMODEL_NAME = 'Qwen/Qwen3-4B-Instruct-2507'\nRANDOM_SEED = 42\nBATCH_SIZE = 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = load_dataset(DATASET_NAME)['train']\nprint(data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"augmentations = ['shift', 'orfo', 'typo', 'delete', 'insert', 'multiply', 'swap']\nchar_aug = CharAug(\n    unit_prob=0.3, # Percentage of the phrase to which augmentations will be applied\n    min_aug=1, # Minimum number of augmentations\n    max_aug=5, # Maximum number of augmentations\n    mult_num=3, # Maximum number of repetitions of characters (only for the multiply method)\n    lang='eng',\n    platform='pc',\n    random_seed=RANDOM_SEED,\n    )\naug_number = 2\n\ndef add_lexical(example):\n    example['lexical'] = [char_aug.augment(text=example['response'], action=random.choice(augmentations)) \n                          for _ in range(aug_number)]\n    return example\n\ndata = data.map(add_lexical, desc='Generate lexical paraphrases')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype='auto', device_map='auto')\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"paraphrase_number = 3\n\ndef prompt_builder(parafrase):\n    prompt = f'''\nYou are a helpful assistant that generates high-quality paraphrases.\nParaphrases must preserve the original meaning and factual content.\nDo not add new information.\nUse different wording and sentence structures.\n\nOriginal:\nThe model was trained on a large dataset.\n\nParaphrases:\n1. The model was trained using a large amount of data.\n2. A large dataset was used to train the model.\n3. The model learned from a very large collection of data.\n\nOriginal:\nVirgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.\n\nParaphrases:\n1. Virgin Australia began operations on 31 August 2000 under the name Virgin Blue, operating two aircraft on a single route. \n2. The airline launched on 31 August 2000 as Virgin Blue, starting with just two planes and one route.\n3. Operations began on 31 August 2000 when the company, then known as Virgin Blue, entered service with two aircraft on a single route.\n\nOriginal:\nThe system returns an error when the input format is incorrect.\n\nParaphrases:\n1. The system produces an error if the input format is invalid.\n2. An incorrect input format causes the system to return an error.\n3. The system fails with an error when the input format is wrong.\n\nOriginal:\n{parafrase}\n\nReturn exactly {paraphrase_number} paraphrases as a JSON array of strings.\nOnly return valid JSON, do not include numbers or any additional text.\n    '''\n    return prompt\n\ndef get_llm_answer(prompt, max_tokens=4096, temperature=0.7, top_p=0.9, do_sample=True):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n    generated_ids = model.generate(\n        **model_inputs,\n        max_new_tokens=max_tokens,\n        do_sample=do_sample,\n        temperature=temperature,\n        top_p=top_p\n    )\n    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n    response = tokenizer.decode(output_ids, skip_special_tokens=True)\n\n    return response\n\ndef add_semantic(example):\n    try: \n        answer = json.loads(get_llm_answer(prompt_builder(example['response']))) \n        example['semantic'] = answer \n        return example \n    except Exception as e: \n        print(f'Что то пошло не так: {e}') \n        example['semantic'] = []    \n        return example","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_list = data.to_list()\nfor i, row in tqdm(enumerate(data_list), total=len(data_list), desc='Generate semantic paraphrases'):\n    try:\n        answer = json.loads(get_llm_answer(prompt_builder(row['response'])))\n        print(answer)\n        data_list[i]['semantic'] = answer\n    except Exception as e:\n        print(f'Что то пошло не так: {e}')\n        data_list[i]['semantic'] = []","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}