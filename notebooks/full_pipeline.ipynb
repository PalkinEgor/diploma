{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e547367f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\envs\\diploma_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc906893",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DECODER_NAME = 'Qwen/Qwen2.5-0.5B'\n",
    "ENCODER_NAME = 'FacebookAI/xlm-roberta-base'\n",
    "CHECKPOINT_PATH = 'D:\\\\diploma\\\\checkpoints\\\\best_model.pt'\n",
    "DATASET_NAME = 'databricks/databricks-dolly-15k'\n",
    "OUTPUT_DIM = 896\n",
    "MIN_WORDS = 5\n",
    "MAX_WORDS = 200\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b095828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "    \n",
    "def collate_fn(batch, tokenizer, device):\n",
    "    input_ids = [tokenizer(text, add_special_tokens=True, return_tensors='pt', truncation=True, max_length=tokenizer.model_max_length)['input_ids'].reshape(-1) for text in batch]\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long().to(device)\n",
    "    return {\n",
    "        'texts': batch,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "    \n",
    "# Класс кастомной модели\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, model_name, output_dim, freeze_bert=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Bert енкодер\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Проекционная голова для e вектора\n",
    "        self.e_proj = torch.nn.Linear(self.bert.config.hidden_size, output_dim)\n",
    "\n",
    "        # Проекционная голова для m вектора\n",
    "        self.m_proj = torch.nn.Linear(self.bert.config.hidden_size, output_dim)\n",
    "\n",
    "        # Проекционная голова для среднего значения распределения длин\n",
    "        self.mu = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "        # Проекционная голова для стандартного отклонения распределения длин\n",
    "        self.std = torch.nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "        # Заморозка модели\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        out = self.bert(input_ids, attention_mask)\n",
    "        cls = out.last_hidden_state[:, 0, :]\n",
    "        e = self.e_proj(cls)\n",
    "        m = self.m_proj(cls)\n",
    "        mu = self.mu(cls)\n",
    "        std = self.std(cls)\n",
    "        return e, m, mu, std\n",
    "\n",
    "# Формирование длин овтетов    \n",
    "def get_lengths(mu, std, strategy):\n",
    "    std = torch.exp(0.5 * std)\n",
    "    if strategy == 'sample':\n",
    "        lengths = torch.normal(mean=mu, std=std)\n",
    "        lengths = torch.clamp(lengths, min=mu - 2 * std, max=mu + 2 * std)\n",
    "    if strategy == 'mean':\n",
    "        lengths = mu\n",
    "    lengths = torch.clamp(lengths.round(), min=1).long()\n",
    "    return lengths\n",
    "\n",
    "# Создание схемы с одним e вектором и text_length - 1 m векторов\n",
    "def generate_input_one(vectors, text_length):\n",
    "    return torch.cat([vectors[:1, None, :], vectors[1:2, None, :].expand(-1, text_length - 1, -1)], dim=1)\n",
    "\n",
    "# Создание схемы для целого батча\n",
    "def generate_input(batch_vectors, lengths):\n",
    "    embeds = []\n",
    "    for vectors, length in zip(batch_vectors, lengths):\n",
    "        embeds.append(generate_input_one(vectors, length).squeeze(0))\n",
    "    return pad_sequence(embeds, batch_first=True, padding_value=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "decoder_model = AutoModelForCausalLM.from_pretrained(DECODER_NAME).to(DEVICE)\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(DECODER_NAME)\n",
    "encoder_model = Model(model_name=ENCODER_NAME, output_dim=OUTPUT_DIM).to(DEVICE)\n",
    "encoder_model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))\n",
    "encoder_tokenizer = AutoTokenizer.from_pretrained(ENCODER_NAME, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d2e8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "df = dataset['train'].to_pandas()\n",
    "df = df[df['response'].apply(lambda x: len(x.split(' ')) > MIN_WORDS)].reset_index(drop=True)\n",
    "texts = list(df['response'])\n",
    "texts = [' '.join(text.split(' ')[:MAX_WORDS]) if len(text.split(' ')) > MAX_WORDS else text for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c85400f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TextDataset(texts)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda x: collate_fn(x, encoder_tokenizer, DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac0397c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset:  12%|█▏        | 212/1701 [00:19<02:18, 10.71it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m max_len = torch.max(lengths)\n\u001b[32m      9\u001b[39m vectors = torch.stack([e, m], dim=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m current_input = \u001b[43mgenerate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[32m     12\u001b[39m attention_mask = torch.arange(max_len, device=lengths.device).expand(BATCH_SIZE, max_len) < lengths\n\u001b[32m     14\u001b[39m logits = decoder_model(inputs_embeds=current_input, attention_mask=attention_mask).logits\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mgenerate_input\u001b[39m\u001b[34m(batch_vectors, lengths)\u001b[39m\n\u001b[32m     70\u001b[39m embeds = []\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m vectors, length \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_vectors, lengths):\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     embeds.append(\u001b[43mgenerate_input_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m.squeeze(\u001b[32m0\u001b[39m))\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pad_sequence(embeds, batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value=\u001b[32m0.0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mgenerate_input_one\u001b[39m\u001b[34m(vectors, text_length)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_input_one\u001b[39m(vectors, text_length):\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat([vectors[:\u001b[32m1\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, :], \u001b[43mvectors\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m], dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "corpus_preds = []\n",
    "for batch in tqdm(test_dataloader, desc='Processing dataset'):\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "\n",
    "    e, m, mu, std = encoder_model(input_ids, attention_mask)\n",
    "    lengths = get_lengths(mu, std, 'mean')\n",
    "    max_len = torch.max(lengths)\n",
    "    vectors = torch.stack([e, m], dim=1)\n",
    "\n",
    "    current_input = generate_input(vectors, lengths)    \n",
    "    attention_mask = torch.arange(max_len, device=lengths.device).expand(BATCH_SIZE, max_len) < lengths\n",
    "\n",
    "    logits = decoder_model(inputs_embeds=current_input, attention_mask=attention_mask).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    preds = decoder_tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    preds = [pred.split(' ') for pred in preds]\n",
    "    corpus_preds.extend(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058d30f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[text.split(' ')] for text in texts]\n",
    "bleu_score_corpus = corpus_bleu(texts, corpus_preds)\n",
    "print(f'Corpus BLEU Score: {bleu_score_corpus}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
